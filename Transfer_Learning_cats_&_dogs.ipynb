{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6UThQUeF60h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= CONFIGURA√á√ÉO DO GOOGLE DRIVE =============\n",
        "def configurar_google_drive():\n",
        "    \"\"\"Monta o Google Drive e configura os caminhos\"\"\"\n",
        "\n",
        "    # Montar Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Configurar caminho para o dataset no Google Drive\n",
        "    BASE_DIR = \"/content/drive/MyDrive/PetImages\"\n",
        "\n",
        "    print(f\"Google Drive montado!\")\n",
        "    print(f\"Caminho do dataset: {BASE_DIR}\")\n",
        "\n",
        "    # Verificar se as pastas existem\n",
        "    cat_dir = os.path.join(BASE_DIR, \"Cat\")\n",
        "    dog_dir = os.path.join(BASE_DIR, \"Dog\")\n",
        "\n",
        "    if not os.path.exists(BASE_DIR):\n",
        "        print(f\"‚ùå ERRO: Pasta {BASE_DIR} n√£o encontrada!\")\n",
        "        print(\"üìã Certifique-se de que voc√™:\")\n",
        "        print(\"   1. Fez upload da pasta PetImages para 'Meu Drive' no Google Drive\")\n",
        "        print(\"   2. A estrutura est√°: MyDrive/PetImages/Cat/ e MyDrive/PetImages/Dog/\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(cat_dir):\n",
        "        print(f\"‚ùå ERRO: Pasta Cat n√£o encontrada em {cat_dir}\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(dog_dir):\n",
        "        print(f\"‚ùå ERRO: Pasta Dog n√£o encontrada em {dog_dir}\")\n",
        "        return None\n",
        "\n",
        "    print(\"‚úÖ Pastas encontradas com sucesso!\")\n",
        "    return BASE_DIR\n",
        "\n",
        "def otimizar_performance_gdrive():\n",
        "    \"\"\"Configura√ß√µes para otimizar performance com Google Drive\"\"\"\n",
        "\n",
        "    # Configurar TensorFlow para usar menos RAM\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Permitir crescimento de mem√≥ria da GPU\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"‚úÖ GPU configurada com crescimento de mem√≥ria din√¢mico\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Erro ao configurar GPU: {e}\")\n",
        "\n",
        "    # Configurar para usar cache em disco (acelera leitura do Drive)\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduzir logs\n",
        "\n",
        "    print(\"‚ö° Configura√ß√µes de performance aplicadas\")\n",
        "\n",
        "# ============= CONFIGURA√á√ïES =============\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32  # Pode reduzir para 16 se der erro de mem√≥ria\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "def verificar_dataset(base_dir):\n",
        "    \"\"\"Verifica a estrutura do dataset\"\"\"\n",
        "    cat_dir = os.path.join(base_dir, \"Cat\")\n",
        "    dog_dir = os.path.join(base_dir, \"Dog\")\n",
        "\n",
        "    # Contar arquivos de imagem\n",
        "    extensoes_validas = ('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG')\n",
        "\n",
        "    num_cats = len([f for f in os.listdir(cat_dir)\n",
        "                   if f.endswith(extensoes_validas)])\n",
        "    num_dogs = len([f for f in os.listdir(dog_dir)\n",
        "                   if f.endswith(extensoes_validas)])\n",
        "\n",
        "    print(f\"üìä Estat√≠sticas do Dataset:\")\n",
        "    print(f\"   üê± Imagens de gatos: {num_cats:,}\")\n",
        "    print(f\"   üê∂ Imagens de cachorros: {num_dogs:,}\")\n",
        "    print(f\"   üìÅ Total de imagens: {num_cats + num_dogs:,}\")\n",
        "\n",
        "    # Verificar balanceamento\n",
        "    ratio = min(num_cats, num_dogs) / max(num_cats, num_dogs)\n",
        "    if ratio < 0.8:\n",
        "        print(f\"‚ö†Ô∏è  Dataset desbalanceado (ratio: {ratio:.2f})\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Dataset bem balanceado (ratio: {ratio:.2f})\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def limpar_dataset_rapido(base_dir):\n",
        "    \"\"\"Vers√£o otimizada para remover imagens corrompidas\"\"\"\n",
        "    import cv2\n",
        "\n",
        "    print(\"üßπ Limpando dataset (removendo imagens corrompidas)...\")\n",
        "\n",
        "    for class_name in ['Cat', 'Dog']:\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        arquivos = [f for f in os.listdir(class_dir)\n",
        "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        removed_count = 0\n",
        "        total_files = len(arquivos)\n",
        "\n",
        "        print(f\"   Verificando {total_files} imagens de {class_name}...\")\n",
        "\n",
        "        for i, filename in enumerate(arquivos):\n",
        "            if i % 1000 == 0:  # Progresso a cada 1000 imagens\n",
        "                print(f\"   Progresso: {i}/{total_files}\")\n",
        "\n",
        "            filepath = os.path.join(class_dir, filename)\n",
        "            try:\n",
        "                # Verifica√ß√£o r√°pida de tamanho\n",
        "                if os.path.getsize(filepath) < 1024:  # Menor que 1KB\n",
        "                    os.remove(filepath)\n",
        "                    removed_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Verifica√ß√£o b√°sica com OpenCV\n",
        "                img = cv2.imread(filepath)\n",
        "                if img is None:\n",
        "                    os.remove(filepath)\n",
        "                    removed_count += 1\n",
        "\n",
        "            except Exception:\n",
        "                try:\n",
        "                    os.remove(filepath)\n",
        "                    removed_count += 1\n",
        "                except:\n",
        "                    pass  # Arquivo pode ter sido removido por outro processo\n",
        "\n",
        "        print(f\"   ‚úÖ {class_name}: {removed_count} imagens corrompidas removidas\")\n",
        "\n",
        "def criar_data_generators_otimizado(base_dir):\n",
        "    \"\"\"Cria geradores de dados otimizados para Google Drive\"\"\"\n",
        "\n",
        "    # Data augmentation para treino - mais conservador para economizar tempo\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Apenas normaliza√ß√£o para valida√ß√£o\n",
        "    val_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Geradores com configura√ß√µes otimizadas\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        base_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='training',\n",
        "        seed=42,\n",
        "        shuffle=True,\n",
        "        # Importante: usar interpola√ß√£o mais r√°pida\n",
        "        interpolation='bilinear'\n",
        "    )\n",
        "\n",
        "    validation_generator = val_datagen.flow_from_directory(\n",
        "        base_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        subset='validation',\n",
        "        seed=42,\n",
        "        shuffle=False,\n",
        "        interpolation='bilinear'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "def criar_modelo_transfer_learning(base_model_name='ResNet50'):\n",
        "    \"\"\"Cria modelo usando transfer learning\"\"\"\n",
        "\n",
        "    print(f\"üèóÔ∏è  Criando modelo base: {base_model_name}\")\n",
        "\n",
        "    # Escolher modelo base\n",
        "    if base_model_name == 'VGG16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False,\n",
        "                          input_shape=(*IMG_SIZE, 3))\n",
        "    elif base_model_name == 'ResNet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False,\n",
        "                             input_shape=(*IMG_SIZE, 3))\n",
        "    elif base_model_name == 'MobileNetV2':\n",
        "        base_model = MobileNetV2(weights='imagenet', include_top=False,\n",
        "                                input_shape=(*IMG_SIZE, 3))\n",
        "    else:\n",
        "        raise ValueError(f\"Modelo base '{base_model_name}' n√£o suportado\")\n",
        "\n",
        "    # Congelar as camadas do modelo base inicialmente\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Criar o modelo completo\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),  # Adicionar batch normalization\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "def treinar_modelo(model, train_gen, val_gen, phase=\"initial\"):\n",
        "    \"\"\"Treina o modelo com configura√ß√µes otimizadas\"\"\"\n",
        "\n",
        "    print(f\"üöÄ Iniciando treinamento - {phase}\")\n",
        "\n",
        "    # Compilar o modelo\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks otimizados\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=8,  # Reduzido para economizar tempo\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.3,\n",
        "            patience=4,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            f'/content/drive/MyDrive/best_model_{phase}.h5',  # Salvar no Drive\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Determinar n√∫mero de √©pocas\n",
        "    epochs = EPOCHS if phase == \"initial\" else EPOCHS // 3\n",
        "\n",
        "    # Calcular steps por √©poca (importante para grandes datasets)\n",
        "    steps_per_epoch = train_gen.samples // BATCH_SIZE\n",
        "    validation_steps = val_gen.samples // BATCH_SIZE\n",
        "\n",
        "    print(f\"   Steps por √©poca: {steps_per_epoch}\")\n",
        "    print(f\"   Validation steps: {validation_steps}\")\n",
        "\n",
        "    # Treinar\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def fine_tuning(model, base_model, train_gen, val_gen):\n",
        "    \"\"\"Realiza fine-tuning desbloqueando algumas camadas\"\"\"\n",
        "\n",
        "    print(\"üîß Iniciando Fine-tuning...\")\n",
        "\n",
        "    # Descongelar as √∫ltimas camadas do modelo base\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Configura√ß√£o mais conservadora para fine-tuning\n",
        "    fine_tune_at = int(len(base_model.layers) * 0.7)  # Descongelar √∫ltimos 30%\n",
        "\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    trainable_layers = sum([layer.trainable for layer in model.layers])\n",
        "    print(f\"   üéØ Camadas trein√°veis ap√≥s fine-tuning: {trainable_layers}\")\n",
        "\n",
        "    # Recompilar com learning rate muito menor\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE/20),  # Learning rate ainda menor\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Treinar com fine-tuning\n",
        "    history_ft = treinar_modelo(model, train_gen, val_gen, phase=\"fine_tuning\")\n",
        "\n",
        "    return history_ft\n",
        "\n",
        "def plotar_historico(history, history_ft=None):\n",
        "    \"\"\"Plota gr√°ficos de loss e accuracy\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)\n",
        "    if history_ft:\n",
        "        epochs_ft = len(history.history['loss'])\n",
        "        axes[0].plot(range(epochs_ft, epochs_ft + len(history_ft.history['loss'])),\n",
        "                    history_ft.history['loss'], label='Treino (Fine-tuning)', linewidth=2)\n",
        "        axes[0].plot(range(epochs_ft, epochs_ft + len(history_ft.history['val_loss'])),\n",
        "                    history_ft.history['val_loss'], label='Valida√ß√£o (Fine-tuning)', linewidth=2)\n",
        "    axes[0].set_title('üìâ Loss Durante o Treinamento', fontsize=14)\n",
        "    axes[0].set_xlabel('√âpocas')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    axes[1].plot(history.history['accuracy'], label='Treino', linewidth=2)\n",
        "    axes[1].plot(history.history['val_accuracy'], label='Valida√ß√£o', linewidth=2)\n",
        "    if history_ft:\n",
        "        epochs_ft = len(history.history['accuracy'])\n",
        "        axes[1].plot(range(epochs_ft, epochs_ft + len(history_ft.history['accuracy'])),\n",
        "                    history_ft.history['accuracy'], label='Treino (Fine-tuning)', linewidth=2)\n",
        "        axes[1].plot(range(epochs_ft, epochs_ft + len(history_ft.history['val_accuracy'])),\n",
        "                    history_ft.history['val_accuracy'], label='Valida√ß√£o (Fine-tuning)', linewidth=2)\n",
        "    axes[1].set_title('üìà Accuracy Durante o Treinamento', fontsize=14)\n",
        "    axes[1].set_xlabel('√âpocas')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def avaliar_modelo(model, val_gen):\n",
        "    \"\"\"Avalia o modelo final\"\"\"\n",
        "\n",
        "    print(\"üìä Avaliando modelo final...\")\n",
        "\n",
        "    # Reset do gerador\n",
        "    val_gen.reset()\n",
        "\n",
        "    # Predi√ß√µes\n",
        "    predictions = model.predict(val_gen, verbose=1)\n",
        "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Labels verdadeiros\n",
        "    true_classes = val_gen.classes[:len(predicted_classes)]  # Garantir mesmo tamanho\n",
        "    class_labels = list(val_gen.class_indices.keys())\n",
        "\n",
        "    # Relat√≥rio de classifica√ß√£o\n",
        "    print(\"\\nüìã Relat√≥rio de Classifica√ß√£o:\")\n",
        "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
        "\n",
        "    # Matriz de confus√£o\n",
        "    cm = confusion_matrix(true_classes, predicted_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.title('üéØ Matriz de Confus√£o')\n",
        "    plt.ylabel('Real')\n",
        "    plt.xlabel('Predito')\n",
        "    plt.show()\n",
        "\n",
        "    # Accuracy final\n",
        "    accuracy = np.mean(predicted_classes == true_classes)\n",
        "    print(f\"\\nüéâ Accuracy final: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fun√ß√£o principal otimizada para Google Drive\"\"\"\n",
        "\n",
        "    print(\"üöÄ === TRANSFER LEARNING - CATS VS DOGS (Google Drive) ===\\n\")\n",
        "\n",
        "    # 1. Configurar Google Drive\n",
        "    print(\"üìÅ Configurando acesso ao Google Drive...\")\n",
        "    base_dir = configurar_google_drive()\n",
        "    if base_dir is None:\n",
        "        return None\n",
        "\n",
        "    # 2. Otimizar performance\n",
        "    print(\"\\n‚ö° Otimizando configura√ß√µes de performance...\")\n",
        "    otimizar_performance_gdrive()\n",
        "\n",
        "    # 3. Verificar dataset\n",
        "    print(f\"\\nüîç Verificando dataset em {base_dir}...\")\n",
        "    if not verificar_dataset(base_dir):\n",
        "        return None\n",
        "\n",
        "    # 4. Limpar dataset (opcional - pode pular se o dataset j√° est√° limpo)\n",
        "    resposta = input(\"\\nü§î Deseja limpar o dataset (remover imagens corrompidas)? [s/N]: \")\n",
        "    if resposta.lower() in ['s', 'sim', 'y', 'yes']:\n",
        "        limpar_dataset_rapido(base_dir)\n",
        "\n",
        "    # 5. Criar geradores de dados\n",
        "    print(\"\\nüîÑ Criando geradores de dados...\")\n",
        "    train_gen, val_gen = criar_data_generators_otimizado(base_dir)\n",
        "\n",
        "    print(f\"   Classes: {train_gen.class_indices}\")\n",
        "    print(f\"   üèãÔ∏è  Amostras de treino: {train_gen.samples:,}\")\n",
        "    print(f\"   ‚úÖ Amostras de valida√ß√£o: {val_gen.samples:,}\")\n",
        "\n",
        "    # 6. Criar modelo\n",
        "    modelo_escolhido = 'ResNet50'  # Pode alterar para 'VGG16' ou 'MobileNetV2'\n",
        "    print(f\"\\nüèóÔ∏è  Criando modelo com transfer learning ({modelo_escolhido})...\")\n",
        "    model, base_model = criar_modelo_transfer_learning(modelo_escolhido)\n",
        "\n",
        "    total_params = model.count_params()\n",
        "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "    print(f\"   üìä Par√¢metros totais: {total_params:,}\")\n",
        "    print(f\"   üéØ Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "\n",
        "    # 7. Primeira fase de treinamento\n",
        "    print(\"\\nüöÄ === FASE 1: Treinamento Inicial ===\")\n",
        "    history1 = treinar_modelo(model, train_gen, val_gen, \"initial\")\n",
        "\n",
        "    # 8. Fine-tuning\n",
        "    resposta = input(\"\\nü§î Deseja fazer fine-tuning? [S/n]: \")\n",
        "    if resposta.lower() not in ['n', 'no', 'nao']:\n",
        "        print(\"\\nüîß === FASE 2: Fine-tuning ===\")\n",
        "        history2 = fine_tuning(model, base_model, train_gen, val_gen)\n",
        "    else:\n",
        "        history2 = None\n",
        "\n",
        "    # 9. Plotar resultados\n",
        "    print(\"\\nüìà Plotando resultados do treinamento...\")\n",
        "    plotar_historico(history1, history2)\n",
        "\n",
        "    # 10. Avaliar modelo final\n",
        "    print(\"\\nüéØ === AVALIA√á√ÉO FINAL ===\")\n",
        "    accuracy = avaliar_modelo(model, val_gen)\n",
        "\n",
        "    # 11. Salvar modelo final\n",
        "    modelo_path = '/content/drive/MyDrive/modelo_cats_dogs_final.h5'\n",
        "    model.save(modelo_path)\n",
        "    print(f\"\\nüíæ Modelo salvo em: {modelo_path}\")\n",
        "    print(f\"üèÜ Accuracy final: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0560JbCCF_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar apenas se o script for executado diretamente\n",
        "if __name__ == \"__main__\":\n",
        "    # Verificar se OpenCV est√° instalado\n",
        "    try:\n",
        "        import cv2\n",
        "\n",
        "        print(\"‚úÖ OpenCV encontrado\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå OpenCV n√£o encontrado. Instalando...\")\n",
        "        !pip install opencv-python\n",
        "        import cv2\n",
        "\n",
        "    # Executar fun√ß√£o principal\n",
        "    modelo_final = main()"
      ],
      "metadata": {
        "id": "858_czPoGIKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}